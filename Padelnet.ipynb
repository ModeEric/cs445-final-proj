{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73015efd-5d55-4603-a900-4be93b8ee919",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2ae05-74cb-4b89-af92-4b3179fd1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  \n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8f220-f06d-457e-9b17-15ad4af38b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714864a-3d52-4621-b0fb-83a13c0d6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadelNetDataset(Dataset):\n",
    "    def __init__(self, base_path, frame_info=3, resize_size=(360,640), transform=None):\n",
    "        super().__init__()\n",
    "        self.base_path = base_path\n",
    "        self.new_H, self.new_W = resize_size\n",
    "        self.frame_info = frame_info\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((resize_size[0], resize_size[1])),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.data = []\n",
    "        self.label = pd.read_csv(os.path.join(base_path, 'Label.csv'))\n",
    "        clips = [f for f in sorted(os.listdir(base_path)) if f.endswith('jpg')]\n",
    "        for clip_name in clips:\n",
    "            clip_path = os.path.join(base_path, clip_name)\n",
    "\n",
    "            label_df = pd.read_csv(os.path.join(base_path, 'Label.csv'))\n",
    "            image_names = label_df['file name'].tolist()\n",
    "\n",
    "            if len(image_names) < frame_info:\n",
    "                continue\n",
    "\n",
    "            for idx in range(frame_info-1, len(image_names)):\n",
    "                self.data.append({\n",
    "                    'clip_path': clip_path,\n",
    "                    'label_df': label_df,\n",
    "                    'center_idx': idx,\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        clip_path = entry['clip_path']\n",
    "        label_df = entry['label_df']\n",
    "        center_idx = entry['center_idx']\n",
    "        imgs = []\n",
    "        for i in range(center_idx - self.frame_info + 1, center_idx + 1):\n",
    "            img_name = label_df.iloc[i]['file name']\n",
    "            img_path = os.path.join(self.base_path, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if i == center_idx:\n",
    "                orig_W, orig_H = img.size\n",
    "            if self.transform: \n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "           \n",
    "        imgs = torch.cat(imgs, dim=0)  \n",
    "        visibility = label_df.iloc[center_idx]['visibility']\n",
    "        x = label_df.iloc[center_idx]['x-coordinate']\n",
    "        y = label_df.iloc[center_idx]['y-coordinate']\n",
    "        if math.isnan(x) or math.isnan(y):\n",
    "            x_resized, y_resized = -1, -1\n",
    "        else:\n",
    "            x_resized = x * (self.new_W / orig_W)\n",
    "            y_resized = y * (self.new_H / orig_H)\n",
    "\n",
    "        target = torch.tensor([visibility, x_resized, y_resized], dtype=torch.float32)\n",
    "\n",
    "        return imgs, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae06882-5fff-4634-8c4f-5b3afafb3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './padel_dataset/final_testing'\n",
    "resize_size = (128, 256)\n",
    "batch_size = 16\n",
    "frame_info = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9116b-cadb-4f26-95ec-58ce1a435268",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PadelNetDataset(test_path, resize_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b6936-ee0d-444b-b0ae-b7afd46d823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725a335-164e-4da8-9cfe-82710add4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class BallTrackerNet(nn.Module):\n",
    "    def __init__(self, frame_info=3, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        # VGG16:generate the feature map\n",
    "        self.VGG16 = nn.Sequential(\n",
    "            ConvBlock(in_channels=frame_info*3, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ConvBlock(in_channels=64, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ConvBlock(in_channels=128, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ConvBlock(in_channels=256, out_channels=512),\n",
    "            ConvBlock(in_channels=512, out_channels=512),\n",
    "            ConvBlock(in_channels=512, out_channels=512)\n",
    "        )\n",
    "            # DeconvNet\n",
    "        self.deconvnet = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ConvBlock(in_channels=512, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=256),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ConvBlock(in_channels=256, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ConvBlock(in_channels=128, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=self.out_channels)\n",
    "        )\n",
    "        self._init_weights()\n",
    "                  \n",
    "    def forward(self, x): \n",
    "        batch_size = x.size(0)\n",
    "        x = self.VGG16(x)\n",
    "        x = self.deconvnet(x)\n",
    "        out = x\n",
    "        return out                       \n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.uniform_(module.weight, -0.05, 0.05)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfc044-ceb1-48cc-af60-21e5097c4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(targets, H=360, W=640, sigma2=10):\n",
    "    \"\"\"\n",
    "    targets: (batch_size, 3) -> (visibility, x, y)\n",
    "    H, W: heatmap\n",
    "    sigma: \n",
    "    \"\"\"\n",
    "    batch_size = targets.shape[0]\n",
    "    heatmaps = torch.zeros((batch_size, 1, H, W), device=targets.device)\n",
    "    for i in range(batch_size):\n",
    "        visibility, x, y = targets[i]\n",
    "        if visibility != 0:\n",
    "            yy, xx = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "            xx = xx.float()\n",
    "            yy = yy.float()\n",
    "            center_x = x.clone().detach()\n",
    "            center_y = y.clone().detach()\n",
    "            heatmap = torch.exp(-((xx - center_x)**2 + (yy - center_y)**2) / (2 * sigma2))\n",
    "            heatmap = heatmap / heatmap.max()  # normalize\n",
    "            heatmap = heatmap.clone().detach()\n",
    "            heatmaps[i, 0] = heatmap\n",
    "\n",
    "    label = (heatmaps*255).long()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cf2e9-d7c5-4fd0-b0b4-ac74b12810df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from scipy.spatial import distance\n",
    "def postprocess_heatmap(heatmap, scale=1):\n",
    "    \"\"\"from heatmap using HoughCircles collect (x,y)\"\"\"\n",
    "    heatmap = heatmap * 255\n",
    "    heatmap = heatmap.astype(np.uint8)\n",
    "    _, binary = cv2.threshold(heatmap, 127, 255, cv2.THRESH_BINARY)\n",
    "    circles = cv2.HoughCircles(binary, cv2.HOUGH_GRADIENT, dp=1, minDist=1,\n",
    "                               param1=50, param2=2, minRadius=2, maxRadius=7)\n",
    "    if circles is not None and len(circles[0]) == 1:\n",
    "        x = int(circles[0][0][0] * scale)\n",
    "        y = int(circles[0][0][1] * scale)\n",
    "        return x, y\n",
    "    return None, None\n",
    "\n",
    "def validate(model, val_loader, criterion, min_dist=5):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp = 0.0\n",
    "    fp = 0.0\n",
    "    tn = 0.0\n",
    "    fn = 0.0\n",
    "    # batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for iter_id, (images, targets) in enumerate(val_loader):\n",
    "            images = images.to(device)          # (B, 9, H, W)\n",
    "            targets = targets.to(device)\n",
    "            gt = generate_heatmap(targets,H=resize_size[0], W=resize_size[1]).squeeze(1)\n",
    "            x_gt_batch = targets[:,1]  # (B,)\n",
    "            y_gt_batch = targets[:, 2]  # (B,)\n",
    "            vis_batch = targets[:, 0]   # (B,)\n",
    "\n",
    "            outputs = model(images)                         # (B, 256, H, W)\n",
    "            loss = criterion(outputs, gt)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(F.softmax(outputs, dim=1), dim=1)  # (B, H, W)\n",
    "            B, H, W = preds.shape\n",
    "            pred_classes_flat = preds.view(B, -1) # (B, HxW)\n",
    "            max_indices = torch.argmax(pred_classes_flat, dim=1)  # (B,)\n",
    "            y_pred = max_indices // W\n",
    "            x_pred = max_indices % W\n",
    "            # L2 distance between predicted and GT coordinates\n",
    "            dist = torch.sqrt((x_pred.float() - x_gt_batch) ** 2 + (y_pred.float() - y_gt_batch) ** 2)\n",
    "\n",
    "            # Logical mask\n",
    "            pred_exists = (pred_classes_flat.max(dim=1).values > 0) \n",
    "            gt_exists = (vis_batch != 0)\n",
    "            tp_mask = pred_exists & gt_exists & (dist < min_dist)\n",
    "            fp_mask = pred_exists & (~gt_exists | (dist >= min_dist))\n",
    "            fn_mask = (~pred_exists) & gt_exists\n",
    "            tn_mask = (~pred_exists) & (~gt_exists)\n",
    "\n",
    "            tp += tp_mask.sum().item()\n",
    "            fp += fp_mask.sum().item()\n",
    "            fn += fn_mask.sum().item()\n",
    "            tn += tn_mask.sum().item()\n",
    "            \n",
    "            # batch_bar.set_postfix(loss1=\"{:.04f}\".format(np.mean(losses)),\n",
    "                                  # tp=tp, tn=tn, fp=fp, fn=fn)\n",
    "            # batch_bar.update()\n",
    "    eps = 1e-15\n",
    "    total = tp + fp + fn + tn\n",
    "    if total == 0:\n",
    "        avg_loss = 0\n",
    "    else:\n",
    "        avg_loss = np.sum(losses) / total\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "\n",
    "    print(f'   Precision = {precision:.4f}')\n",
    "    print(f'   Recall    = {recall:.4f}')\n",
    "    print(f'   F1 Score  = {f1:.4f}')\n",
    "    # batch_bar.close()\n",
    "    return np.mean(losses), precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b046343-d508-4afe-be6d-f2cf08b87583",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97393184-cf88-4c49-876a-a310b03f46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BallTrackerNet()\n",
    "model.load_state_dict(torch.load('model_best.pth', weights_only=True))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22222c-856c-4ca1-bce1-6d2e932eb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist, precision, recall, f1 = validate(model, test_loader, criterion)\n",
    "print(\"Val dist {:.04f} \\t precision: {:.04f} \\t recall: {:.04f}\\t f1: {:.04f}\".format(\n",
    "    test_dist, precision, recall, f1\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
